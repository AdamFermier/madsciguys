---
title: Aggregation
weight: 4
description: Strategies and best practices for aggregating diverse pharmaceutical data sources into unified, analyzable, and reproducible datasets.
---

## Aggregation Strategies Come in Many Shapes and Sizes

Once data has been collected, it often exists in a variety of formats—PDFs, CSVs, Excel files, databases, and more. The challenge is to bring this disparate information together in a way that enables meaningful analysis, supports decision-making, and maintains data integrity.

Aggregation is not a one-size-fits-all process. The optimal strategy depends on your goals, available resources, and the complexity of your data landscape. The focus should always be on fit-for-purpose approaches that optimize speed, cost, and quality.

### Key Considerations for Data Aggregation

- **Conformance to a Common Data Model:** Standardizing data into a unified structure is essential for downstream analytics, regulatory submissions, and reproducibility.
- **Data Cleaning and Transformation:** Raw data often requires cleaning, normalization, and transformation to ensure consistency and usability.
- **Automation vs. Manual Integration:** Automated pipelines can accelerate aggregation and reduce errors, but manual curation may be necessary for complex or legacy datasets.
- **Documentation and Traceability:** Every step in the aggregation process should be well-documented to ensure transparency and reproducibility.

### Recipe-Based Approach

Our recommended strategy is to follow a [recipe-based approach](poulsen-fermier-1.pdf), which provides a structured, repeatable method for aggregating and transforming data. This approach emphasizes modularity, transparency, and adaptability—key principles for robust data management in pharmaceutical research and manufacturing.

For a detailed discussion and practical examples, see the following paper:

{{< iframe-pdf "poulsen-fermier-1.pdf" >}}